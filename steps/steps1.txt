# To create a bucket
aws s3api create-bucket \
    --bucket udagram-474175502926 \
    --region us-east-1

# To create a DB instance
aws rds create-db-instance \
    --db-name udagram \
    --db-instance-identifier postgres \
    --db-instance-class db.t3.micro \
    --engine postgres \
    --master-username postgres \
    --master-user-password diamond25 \
    --allocated-storage 10


aws iam create-role --role-name myAmazonEKSClusterRole --assume-role-policy-document file://"eks-cluster-role-trust-policy.json"

aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy --role-name myAmazonEKSClusterRole

aws iam create-role --role-name AmazonEKSNodeRole --assume-role-policy-document file://"node-role-trust-relationship.json"

aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy --role-name AmazonEKSNodeRole

aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly --role-name AmazonEKSNodeRole

aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy --role-name AmazonEKSNodeRole

can skip the above

# to cleanup resources, run
eksctl delete cluster --region=us-east-1 --name=microservice-cluster

# Feel free to use the same/different flags as you like
# Recommended: You can see many more flags using "eksctl create cluster --help" command.
# For example, you can set the node instance type using --node-type flag
eksctl create cluster --name microservice-cluster --region us-east-1 --version 1.23 --vpc-public-subnets subnet-0ce366f8586ff054c,subnet-046177f11121d4bc8 --nodes-min=2 --nodes-max=3

kubectl get svc

aws ec2 create-key-pair --key-name my-key-pair --key-type rsa --key-format pem --query "KeyMaterial" --output text > my-key-pair.pem

eksctl create nodegroup --cluster microservice-cluster --region us-east-1 --name microservice-node-group --node-type m5.large --nodes 2 --nodes-min 2 --nodes-max 3 --ssh-access --ssh-public-key my-key-pair

kubectl get nodes

cat ~/.aws/credentials | tail -n 5 | head -n 2 | base64

echo "$@2Kuljones" | base64

echo "bWVnYV9zZWNyZXRfa2V5Cg==" | base64 -d

kubectl apply -f aws-secret.yaml
kubectl apply -f env-secret.yaml
kubectl apply -f env-configmap.yaml

kubectl apply -f backend-feed-deployment.yaml
kubectl apply -f backend-feed-service.yaml

kubectl apply -f backend-user-deployment.yaml
kubectl apply -f backend-user-service.yaml

kubectl apply -f frontend-deployment.yaml
kubectl apply -f frontend-service.yaml

kubectl apply -f reverseproxy-deployment.yaml
kubectl apply -f reverseproxy-service.yaml

# Check the deployment names and their pod status
kubectl get deployments

# Create a Service object that exposes the frontend deployment
# The command below will ceates an external load balancer and assigns a fixed, external IP to the Service.
kubectl expose deployment udagram-frontend --type=LoadBalancer --name=publicfrontend
kubectl expose deployment reverseproxy --type=LoadBalancer --name=publicreverseproxy

# Repeat the process for the *reverseproxy* deployment.
# Check name, ClusterIP, and External IP of all deployments
kubectl get services
kubectl get pods
# It should show the STATUS as Running

# publicreverseproxy
a5b0aa8afe6e64e89b171b1c0a1158a7-213045159.us-east-1.elb.amazonaws.com

# Run these commands from the /udagram-deployment directory
# Rolling update the containers of "frontend" deployment

kubectl set image deployment backend-feed backend-feed=ladypatra/udagram-api-feed:v2
kubectl set image deployment backend-user backend-user=ladypatra/udagram-api-user:v2
kubectl set image deployment udagram-frontend udagram-frontend=ladypatra/udagram-frontend:v2
kubectl set image deployment reverseproxy reverseproxy=ladypatra/udagram-reverseproxy:v2


kubectl describe pod [pod-name]

# Deploy the Metrics Server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Verify that the metrics-server deployment is running the desired number of pods with the following command.
kubectl get deployment metrics-server -n kube-system

kubectl get pods
kubectl logs [pod-name] -p
# Once you increase the memory, check the updated deployment as:
kubectl get pod [pod-name] --output=yaml

# You can autoscale, if required, as
kubectl autoscale deployment backend-feed --cpu-percent=70 --min=3 --max=5
kubectl autoscale deployment backend-user --cpu-percent=70 --min=3 --max=5

# delete autoscale
kubectl delete hpa backend-feed

kubectl exec --stdin --tty <pod-name> -- bash

# See what values are set for environment variables in the container
printenv | grep POST
# Or, you can try "curl <cluster-IP-of-backend>:8080/api/v0/feed " to check if services are running.
# This is helpful to see is backend is working by opening a bash into the frontend container
